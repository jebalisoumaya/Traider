{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66089d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ðŸ”¹ 1. Imports & Setup\n",
    "# =============================\n",
    "# !pip install yfinance tensorflow scikit-learn pandas matplotlib joblib --quiet\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "symbol = \"BTC-USD\"\n",
    "interval = \"1h\"   # ChangÃ© de 1m Ã  1h pour avoir plus de donnÃ©es stables\n",
    "period = \"60d\"    # ChangÃ© de 7d Ã  60d pour avoir plus d'historique\n",
    "\n",
    "df = yf.download(tickers=symbol, interval=interval, period=period)\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(f\"Aucune donnÃ©e tÃ©lÃ©chargÃ©e pour {symbol}\")\n",
    "\n",
    "print(f\"{df.shape[0]} lignes tÃ©lÃ©chargÃ©es\")\n",
    "df.head()\n",
    "# =============================\n",
    "# ðŸ”¹ 2. PrÃ©paration des donnÃ©es\n",
    "# =============================\n",
    "\n",
    "# Utiliser seulement le prix de clÃ´ture  \n",
    "data = df[['Close']].astype('float32').values\n",
    "\n",
    "# Normalisation sur toute la sÃ©rie ( )\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "print(f\"DonnÃ©es normalisÃ©es: {data_normalized.shape}\")\n",
    "# =============================\n",
    "# ðŸ”¹ 3. GÃ©nÃ©ration des sÃ©quences avec TimeseriesGenerator\n",
    "# =============================\n",
    "\n",
    "look_back = 72  # MÃªme valeur que \n",
    "split_idx = int(len(data_normalized) * 0.9)\n",
    "\n",
    "train_generator = TimeseriesGenerator(\n",
    "    data_normalized, data_normalized,\n",
    "    length=look_back, batch_size=32,\n",
    "    start_index=0, end_index=split_idx - 1\n",
    ")\n",
    "\n",
    "val_generator = TimeseriesGenerator(\n",
    "    data_normalized, data_normalized,\n",
    "    length=look_back, batch_size=32,\n",
    "    start_index=split_idx - look_back  # permet au premier target de val d'Ãªtre Ã  split_idx\n",
    ")\n",
    "\n",
    "print(f\"GÃ©nÃ©rateur crÃ©Ã© avec look_back={look_back}\")\n",
    "print(f\"Nombre de batches: {len(train_generator)}\")\n",
    "\n",
    "# Supprimer la classe CBAM_Block_1D - pas nÃ©cessaire pour ce cas simple\n",
    "pass\n",
    "# =============================\n",
    "# ðŸ”¹ 4. Construction du modÃ¨le LSTM\n",
    "# =============================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(look_back, 1)),  # SimplifiÃ©: 1 feature au lieu de 3\n",
    "    Dense(1)  # PrÃ©diction d'1 seul pas au lieu de 3\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ 5. EntraÃ®nement (simplifiÃ©)\n",
    "# =============================\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=50, verbose=1)\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ 6. PrÃ©dictions et visualisation\n",
    "# =============================\n",
    "\n",
    "# GÃ©nÃ©rateur pour les prÃ©dictions\n",
    "test_generator = TimeseriesGenerator(\n",
    "    data_normalized, data_normalized,\n",
    "    length=look_back, batch_size=1\n",
    ")\n",
    "\n",
    "# PrÃ©dictions\n",
    "predictions_normalized = model.predict(test_generator)\n",
    "predictions = scaler.inverse_transform(predictions_normalized)\n",
    "\n",
    "# Vraies valeurs (on saute les look_back premiers points)\n",
    "vraies_valeurs = data[look_back:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history.get('loss', []), label='train_loss')\n",
    "\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    \n",
    "plt.title('Courbe de loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(vraies_valeurs, label='Vraies Valeurs', alpha=0.8)\n",
    "plt.plot(predictions, label='PrÃ©dictions', alpha=0.7)\n",
    "plt.title(f\"BTC-USD â€” Vraies vs. PrÃ©dictions (Close) â€” look_back={look_back}\")\n",
    "plt.xlabel('Temps')\n",
    "plt.ylabel('Prix (Close)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcul de quelques mÃ©triques\n",
    "mse = np.mean((vraies_valeurs - predictions)**2)\n",
    "mae = np.mean(np.abs(vraies_valeurs - predictions))\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "\n",
    "# =============================\n",
    "# ðŸ”¹ 7. Sauvegarde du modÃ¨le et du scaler\n",
    "# =============================\n",
    "import joblib\n",
    "\n",
    "# model.save(\"simple_lstm_model.h5\")\n",
    "# joblib.dump({\"scaler\": scaler, \"look_back\": look_back}, \"simple_preproc.pkl\")\n",
    "\n",
    "print(\"âœ… ModÃ¨le et scaler sauvegardÃ©s avec succÃ¨s !\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Baseline : LSTM simple ===\n",
    "# EntraÃ®ne un LSTM simple, prÃ©dit sur test_generator et stocke rÃ©sultats dans preds_lstm\n",
    "try:\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Dependencies missing: ensure tensorflow and sklearn are installed\")\n",
    "\n",
    "# Parameters\n",
    "epochs = 30\n",
    "\n",
    "# build baseline LSTM\n",
    "n_features = 1 if (\"data_normalized\" in globals() and data_normalized.ndim == 2 and data_normalized.shape[1] == 1) else (train_generator[0][0].shape[-1])\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, input_shape=(look_back, n_features)),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model_lstm.summary()\n",
    "\n",
    "# callbacks\n",
    "es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# fit\n",
    "history_lstm = model_lstm.fit(train_generator, validation_data=val_generator, epochs=epochs, callbacks=[es, rlr], verbose=1)\n",
    "\n",
    "# predict on test\n",
    "preds_lstm_norm = model_lstm.predict(test_generator)\n",
    "# invert scaler if available\n",
    "if 'scaler' in globals():\n",
    "    preds_lstm = scaler.inverse_transform(preds_lstm_norm)\n",
    "else:\n",
    "    preds_lstm = preds_lstm_norm\n",
    "\n",
    "# true values from test_generator (inverse scaled)\n",
    "y_test_norm = np.vstack([y for _, y in test_generator])\n",
    "if 'scaler' in globals():\n",
    "    y_test = scaler.inverse_transform(y_test_norm)\n",
    "else:\n",
    "    y_test = y_test_norm\n",
    "\n",
    "print('Baseline LSTM done â€” predictions shape:', preds_lstm.shape, 'true shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hybrid : LSTM + GRU ===\n",
    "# Construire un modÃ¨le combinant LSTM puis GRU (simple) et comparer\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model_hybrid = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(look_back, n_features)),\n",
    "    GRU(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_hybrid.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model_hybrid.summary()\n",
    "\n",
    "history_hybrid = model_hybrid.fit(train_generator, validation_data=val_generator, epochs=epochs, callbacks=[es, rlr], verbose=1)\n",
    "\n",
    "preds_hybrid_norm = model_hybrid.predict(test_generator)\n",
    "if 'scaler' in globals():\n",
    "    preds_hybrid = scaler.inverse_transform(preds_hybrid_norm)\n",
    "else:\n",
    "    preds_hybrid = preds_hybrid_norm\n",
    "\n",
    "print('Hybrid LSTM+GRU done â€” predictions shape:', preds_hybrid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff122e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparaison des modÃ¨les ===\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ensure y_test available\n",
    "try:\n",
    "    y_true = y_test\n",
    "except NameError:\n",
    "    y_test_norm = np.vstack([y for _, y in test_generator])\n",
    "    y_true = scaler.inverse_transform(y_test_norm) if 'scaler' in globals() else y_test_norm\n",
    "\n",
    "mse_lstm = mean_squared_error(y_true, preds_lstm)\n",
    "mae_lstm = mean_absolute_error(y_true, preds_lstm)\n",
    "\n",
    "mse_hybrid = mean_squared_error(y_true, preds_hybrid)\n",
    "mae_hybrid = mean_absolute_error(y_true, preds_hybrid)\n",
    "\n",
    "print('Baseline LSTM -> MSE: %.4f, MAE: %.4f' % (mse_lstm, mae_lstm))\n",
    "print('Hybrid LSTM+GRU -> MSE: %.4f, MAE: %.4f' % (mse_hybrid, mae_hybrid))\n",
    "\n",
    "# plot comparison (first 500 points to keep it readable)\n",
    "nplot = min(500, len(y_true))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_true[:nplot], label='True', color='black')\n",
    "plt.plot(preds_lstm[:nplot], label='LSTM', alpha=0.8)\n",
    "plt.plot(preds_hybrid[:nplot], label='LSTM+GRU', alpha=0.8)\n",
    "plt.legend(); plt.title('True vs LSTM vs LSTM+GRU (test sample)'); plt.show()\n",
    "\n",
    "# small table summary\n",
    "import pandas as pd\n",
    "summary = pd.DataFrame({\n",
    "    'model': ['LSTM', 'LSTM+GRU'],\n",
    "    'mse': [mse_lstm, mse_hybrid],\n",
    "    'mae': [mae_lstm, mae_hybrid]\n",
    "})\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
